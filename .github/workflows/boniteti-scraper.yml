name: Boniteti Daily Scraper

on:
  schedule:
    # Runs every day at 2:00 AM UTC (3:00 AM CET)
    - cron: '0 2 * * *'
  
  workflow_dispatch:
    inputs:
      companies:
        description: 'Number of companies to scrape (leave empty for all)'
        required: false
        type: number
        default: 100
      test_mode:
        description: 'Run in test mode'
        required: false
        type: boolean
        default: false

jobs:
  scrape:
    name: Scrape Boniteti Data
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: boniteti-scraper/package-lock.json
      
      - name: Install dependencies
        working-directory: ./boniteti-scraper
        run: npm ci
      
      - name: Run scraper (scheduled)
        if: github.event_name == 'schedule'
        working-directory: ./boniteti-scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          MODE: production
          LOG_LEVEL: info
          MAX_CONCURRENT_REQUESTS: 10
          REQUEST_DELAY_MS: 1000
        run: |
          npm run scrape-all
      
      - name: Run scraper (manual)
        if: github.event_name == 'workflow_dispatch'
        working-directory: ./boniteti-scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          MODE: ${{ github.event.inputs.test_mode == 'true' && 'test' || 'production' }}
          LOG_LEVEL: ${{ github.event.inputs.test_mode == 'true' && 'debug' || 'info' }}
          MAX_CONCURRENT_REQUESTS: 10
          REQUEST_DELAY_MS: 1000
        run: |
          if [ "${{ github.event.inputs.companies }}" != "" ]; then
            node scraper.js --companies ${{ github.event.inputs.companies }}
          else
            npm run scrape-all
          fi
      
      - name: Upload error logs
        if: failure()
        uses: actions/upload-artifact@v3
        with:
          name: error-logs
          path: boniteti-scraper/errors-*.json
          retention-days: 30
      
      - name: Send notification on failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const issue = await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Boniteti Scraper Failed - ${new Date().toISOString().split('T')[0]}`,
              body: `The daily Boniteti scraper failed. Check the [workflow run](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}) for details.`,
              labels: ['bug', 'scraper']
            });
            console.log(`Created issue #${issue.data.number}`);

  # Optional: Send summary to Slack/Discord
  notify:
    name: Send Summary
    runs-on: ubuntu-latest
    needs: scrape
    if: always()
    
    steps:
      - name: Send summary
        run: |
          if [ "${{ needs.scrape.result }}" == "success" ]; then
            echo "✅ Scraping completed successfully"
          else
            echo "❌ Scraping failed"
          fi